{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProfAIling project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries.\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from spacy import tokenizer\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "tkz = tokenizer.Tokenizer(nlp.vocab)\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "import unicodedata\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = [4, 4]\n",
    "plt.rcParams['font.family'] = 'Serif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pio.templates.default = \"simple_white\"\n",
    "px.defaults.template = \"ggplot2\"\n",
    "our_col = ['#ef476f', '#ffd166', '#06d6a0', '#118ab2', '#073b4c']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 post  age  gender\n",
      "0                           ooh shiny new commenting!   16  female\n",
      "1   so wuts up? today i had the parade. suked. but...   14    male\n",
      "2   i don't know about anyone else anymore, but i'...   24  female\n",
      "3   urlLink    another roof-top sunset  Posted by ...   24    male\n",
      "4   gawd i luv my nanny!  she's absolutely the gre...   23  female\n",
      "5   7._ Km, 39:19.4, Partly cloudy -4C, 6 km wind ...   41    male\n",
      "6   well, it's still summer vacation ':-( and I wi...   13    male\n",
      "7   Yes! School is out for summer! The sun is shin...   17  female\n",
      "8   urlLink Electric Venom:A Venomous Love Note   ...   34  female\n",
      "9   Boring mission. We had another routine presenc...   25    male\n",
      "10  I passed. Details not yet available. But I pas...   23    male\n",
      "11  Cel, this one's for you.  *Ahem*  urlLink Heh ...   27  female\n",
      "12                                           572.8!!!   27    male\n",
      "13  Eight and a half hours, at a laundromat. Gah, ...   17    male\n",
      "14  If you know what the first part of the title m...   25    male\n",
      "\n",
      "(526812, 3)\n"
     ]
    }
   ],
   "source": [
    "# Import data.\n",
    "train = pd.read_json('/Users/simonefacchiano/Desktop/Data Science/SL/Project/train.json')\n",
    "\n",
    "# Let's have a look at out data.\n",
    "print(train.head(15))\n",
    "print('')\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Counts and lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New column for the length of the posts.\n",
    "train['length'] = train['post'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'M DELETING PART OF THE DATA, NEED TO BE DISCUSS!!\n",
    "train = train.loc[train[\"length\"]<80000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to eliminate post with this length\n",
    "fig = px.histogram(train, \n",
    "                   x='length',\n",
    "                   title='Histogram of Packet Length',\n",
    "                   opacity=0.8,\n",
    "                   width=800, \n",
    "                   height=400)\n",
    "fig.update_layout(title_text=\"Length\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missin values.\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target class distribution.\n",
    "#train.age.hist(bins = 15,  color = \"#118ab2\", ec=\"white\", label = \"Age distribution\")\n",
    "fig = px.histogram(train, \n",
    "                   x='age',\n",
    "                   title='Age distribution',\n",
    "                   opacity=0.8,\n",
    "                   width=800, \n",
    "                   height=400,\n",
    "                   color_discrete_sequence = [our_col[0]],\n",
    "                   )\n",
    "fig.update_layout(\n",
    "    yaxis_title_text='Frequency',\n",
    "    bargap=0.01, # gap between bars of adjacent location coordinates\n",
    "    #margin=dict(l=20, r=20, t=20, b=20),\n",
    "    #paper_bgcolor=\"gray\"\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double Check\n",
    "train['age_class'] = pd.cut(\n",
    "        train[\"age\"],\n",
    "        bins=[12, 18, 28, 50],\n",
    "        labels=[0, 1, 2]\n",
    "    ).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train[train.age <= 17]))                        # Class 1\n",
    "print(len(train[(train.age >= 23) & (train.age <= 27)]))  # Class 2\n",
    "print(len(train[(train.age >= 28) & (train.age <= 48)]))  # Class 3\n",
    "\n",
    "#same check\n",
    "train[\"age_class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a new colums with the number of word for the number of words in every post.\n",
    "# Different from Lenght because is not the numbers of characters.\n",
    "train['word_count'] = train['post'].apply(lambda x: len(str(x).split()))\n",
    "print(train[train['age_class']==0]['word_count'].mean()) # 12 - 17 tweets\n",
    "print(train[train['age_class']==1]['word_count'].mean()) # 18 - 29 tweets\n",
    "print(train[train['age_class']==2]['word_count'].mean()) # 29 - 50 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This histograms are only up to 1500 words because few tweets have more than this number but there are some tweet with a loooots of words.\n",
    "train[train.age_class==0]['word_count'].max() #115370 \n",
    "train[train.age_class==1]['word_count'].max() #69208\n",
    "train[train.age_class==2]['word_count'].max() #131169\n",
    "\n",
    "#weeeel very big, maybe they are outliers...\n",
    "word_0 = px.histogram(train[train.age_class==0][train.word_count < 1500]['word_count'],opacity=0.8,color_discrete_sequence = [our_col[0]],nbins=15)\n",
    "word_1 = px.histogram(train[train.age_class==1][train.word_count < 1500]['word_count'],opacity=0.8,color_discrete_sequence = [our_col[1]],nbins=15)\n",
    "word_2 = px.histogram(train[train.age_class==2][train.word_count < 1500]['word_count'],opacity=0.8,color_discrete_sequence = [our_col[2]],nbins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=3,subplot_titles=(\"Class 1\", \"Class 2\", \"Class 3\"),shared_yaxes=True)\n",
    "fig.add_trace(word_0['data'][0], row=1, col=1)\n",
    "fig.add_trace(word_1['data'][0], row=1, col=2)\n",
    "fig.add_trace(word_2['data'][0], row=1, col=3)\n",
    "fig.update_layout(template='ggplot2',bargap=0.1,showlegend=False, height = 400, width = 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence counts\n",
    "train[\"sent_count\"] = train[\"post\"].map(lambda x: len(sent_tokenize(x)))\n",
    "# Average word length\n",
    "train[\"avg_word_len\"] = train[\"post\"].map(lambda x: np.mean([len(w) for w in str(x).split()])).fillna(0)\n",
    "# Average sentence length\n",
    "train[\"avg_sent_len\"] = train[\"post\"].map(lambda x: np.mean([len(w.split()) for w in sent_tokenize(x)])).fillna(0)\n",
    "\n",
    "#take ages to run (8 minutes ahahah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small = train.loc[train[\"length\"]<1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_labels = [\"Class0\",\"Class1\",\"Class2\"]\n",
    "x1_length = train_small[train_small.age_class==0]['length']\n",
    "x2_length = train_small[train_small.age_class==1]['length']\n",
    "x3_length = train_small[train_small.age_class==2]['length']\n",
    "hist_data_length = [x1_length,x2_length,x3_length]\n",
    "\n",
    "x1_word = train_small[train_small.age_class==0][train_small.word_count <= 230]['word_count']\n",
    "x2_word = train_small[train_small.age_class==1][train_small.word_count <= 230]['word_count']\n",
    "x3_word = train_small[train_small.age_class==2][train_small.word_count <= 230]['word_count']\n",
    "hist_data_word = [x1_word,x2_word,x3_word]\n",
    "\n",
    "x1_sent = train_small[train_small.age_class==0][train_small.sent_count <= 30]['sent_count']\n",
    "x2_sent = train_small[train_small.age_class==1][train_small.sent_count <= 30]['sent_count']\n",
    "x3_sent = train_small[train_small.age_class==2][train_small.sent_count <= 30]['sent_count']\n",
    "hist_data_sent = [x1_sent,x2_sent,x3_sent]\n",
    "\n",
    "\n",
    "x1_avg_w = train_small[train_small.age_class==0][train_small.avg_word_len <= 10]['avg_word_len']\n",
    "x2_avg_w = train_small[train_small.age_class==1][train_small.avg_word_len <= 10]['avg_word_len']\n",
    "x3_avg_w = train_small[train_small.age_class==2][train_small.avg_word_len <= 10]['avg_word_len']\n",
    "hist_data_avg_w = [x1_avg_w,x2_avg_w,x3_avg_w]\n",
    "\n",
    "x1_avg_s = train_small[train_small.age_class==0][train_small.avg_sent_len <= 50]['avg_sent_len']\n",
    "x2_avg_s = train_small[train_small.age_class==1][train_small.avg_sent_len <= 50]['avg_sent_len']\n",
    "x3_avg_s = train_small[train_small.age_class==2][train_small.avg_sent_len <= 50]['avg_sent_len']\n",
    "hist_data_avg_s = [x1_avg_s,x2_avg_s,x3_avg_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = ff.create_distplot(hist_data_length, group_labels,show_hist=False, show_rug= False,colors=our_col,show_curve= True)\n",
    "word = ff.create_distplot(hist_data_word, group_labels,show_hist=False, show_rug= False,colors=our_col,show_curve= True)\n",
    "sent = ff.create_distplot(hist_data_sent, group_labels,show_hist=False, show_rug= False,colors=our_col,show_curve= True)\n",
    "avg_word = ff.create_distplot(hist_data_avg_w, group_labels,show_hist=False, show_rug= False,colors=our_col,show_curve= True)\n",
    "avg_sent = ff.create_distplot(hist_data_avg_s, group_labels,show_hist=False, show_rug= False,colors=our_col,show_curve= True)\n",
    "age = ff.create_distplot([train_small.age], group_labels=[\"Age\"],show_hist=False, show_rug= False,colors=[our_col[3]],show_curve= True)\n",
    "\n",
    "#60 seconds to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to avoid too many legends\n",
    "length['data'][0]['showlegend'] = False\n",
    "word['data'][0]['showlegend'] = False\n",
    "sent['data'][0]['showlegend'] = False\n",
    "avg_word['data'][0]['showlegend'] = False\n",
    "\n",
    "length['data'][1]['showlegend'] = False\n",
    "word['data'][1]['showlegend'] = False\n",
    "sent['data'][1]['showlegend'] = False\n",
    "avg_word['data'][1]['showlegend'] = False\n",
    "\n",
    "length['data'][2]['showlegend'] = False\n",
    "word['data'][2]['showlegend'] = False\n",
    "sent['data'][2]['showlegend'] = False\n",
    "avg_word['data'][2]['showlegend'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde = make_subplots(rows=2, cols=3,subplot_titles=(\"Age\", \"Word Count\", \"Sentence Count\", \"Avg word lenght\", \"Avg Sentence length\",\"Length\"))\n",
    "kde.add_trace(length['data'][0], row=2, col=3)\n",
    "kde.add_trace(length['data'][1], row=2, col=3)\n",
    "kde.add_trace(length['data'][2], row=2, col=3)\n",
    "\n",
    "kde.add_trace(word['data'][0], row=1, col=2)\n",
    "kde.add_trace(word['data'][1], row=1, col=2)\n",
    "kde.add_trace(word['data'][2], row=1, col=2)\n",
    "\n",
    "kde.add_trace(sent['data'][0], row=1, col=3)\n",
    "kde.add_trace(sent['data'][1], row=1, col=3)\n",
    "kde.add_trace(sent['data'][2], row=1, col=3)\n",
    "\n",
    "kde.add_trace(avg_word['data'][0], row=2, col=1)\n",
    "kde.add_trace(avg_word['data'][1], row=2, col=1)\n",
    "kde.add_trace(avg_word['data'][2], row=2, col=1)\n",
    "\n",
    "kde.add_trace(avg_sent['data'][0], row=2, col=2)\n",
    "kde.add_trace(avg_sent['data'][1], row=2, col=2)\n",
    "kde.add_trace(avg_sent['data'][2], row=2, col=2)\n",
    "kde.add_trace(age['data'][0], row=1, col=1)\n",
    "kde.update_layout(template='ggplot2')\n",
    "kde.update_layout(\n",
    "    height=1000, \n",
    "    width=1400,\n",
    "    title_text=\"KDE of the variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small_word_count = train_small[train_small.word_count <= 230]\n",
    "train_small_sent_count = train_small[train_small.sent_count <= 30]\n",
    "train_small_avg_word = train_small[train_small.avg_word_len <= 10]\n",
    "train_small_avg_sent = train_small[train_small.avg_sent_len <= 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small_word_count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE plot\n",
    "# (Kernel density estimate)\n",
    "# KDE represents the data using a continuous probability density curve in one or more dimensions.\n",
    "\n",
    "#could be cool but we need to remove outliers... \n",
    "#sns.kdeplot(data = train, x = \"length\", hue = \"age_class\")\n",
    "fig, axes = plt.subplots(figsize=(17,10), ncols=3, nrows=2)\n",
    "sns.kdeplot(data = train_small, x = \"age\", ax = axes[0][0], label = \"Age\", color = our_col[0])\n",
    "sns.kdeplot(data = train_small, x = \"length\", ax = axes[0][1], label = \"Length\", hue=\"age_class\", palette = our_col)\n",
    "sns.kdeplot(data = train_small_word_count, x = \"word_count\", ax = axes[0][2], label = \"Word Count\", hue=\"age_class\", palette = our_col)\n",
    "sns.kdeplot(data = train_small_avg_sent, x = \"avg_sent_len\", ax = axes[1][0], label = \"Average sentence length\", hue=\"age_class\", palette = our_col)\n",
    "sns.kdeplot(data = train_small_avg_word, x = \"avg_word_len\", ax = axes[1][1], label = \"Average word length\", hue=\"age_class\", palette = our_col)\n",
    "sns.kdeplot(data = train_small_sent_count, x = \"sent_count\", ax = axes[1][2], label = \"Sentence count\", hue=\"age_class\", palette = our_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for the variables or violinplots\n",
    "\n",
    "#Ora non c'ho voglia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing | Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before doing anything, we save the original colum of posts in a specifi object that we will keep.\n",
    "original_posts = train['post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also save the original length of the posts... you never know...\n",
    "\n",
    "def post_length(text):\n",
    "    return len(text.split())\n",
    "\n",
    "train['length'] = train['post'].apply(post_length) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0. Expand Contractions\n",
    "\n",
    "We noticed that it is better to expand the contractions at the begginning, before lowering the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/text-normalization-for-natural-language-processing-nlp-70a314bfa646\n",
    "# Not as efficient as other specialized packets... bu at least it works well\n",
    "contractions_dict = {\"ain’t\": \"are not\", \"'s\":\" is\", \" s \":\" is\", \"aren’t\": \"are not\", \"Aren't\": \"are not\", \"can’t\": \"cannot\", \"can’t’ve\": \"cannot have\", \"‘cause\": \"because\", \"could’ve\": \"could have\", \"couldn’t\": \"could not\", \"couldn’t’ve\": \"could not have\", \"didn’t\": \"did not\", \"doesn’t\": \"does not\", \"don't\": \"do not\", \"don t\": \"do not\", \"hadn’t\": \"had not\", \"hadn’t’ve\": \"had not have\", \"hasn’t\": \"has not\", \"haven’t\": \"have not\", \"he’d\": \"he would\", \"he’d’ve\": \"he would have\", \"he’ll\": \"he will\", \"he’ll’ve\": \"he will have\", \"how’d\": \"how did\", \"how’d’y\": \"how do you\", \"how’ll\": \"how will\", \"I’d\": \"I would\", \"I’d’ve\": \"I would have\", \"I’ll\": \"I will\", \"i'll\": \"i will\", \"'ll\":\" will\", \"I’ll’ve\": \"I will have\", \"I’m\": \"I am\", \"i'm\": \"i am\", \"'m\": \" am\", \"im\": \"i am\", \"I’ve\": \"I have\", \"i've\": \"i have\", \"isn’t\": \"is not\", \"it’d\": \"it would\", \"it’d’ve\": \"it would have\", \"it’ll\": \"it will\", \"it’ll’ve\": \"it will have\", \"let’s\": \"let us\", \"ma’am\": \"madam\", \"mayn’t\": \"may not\", \"might’ve\": \"might have\", \"mightn’t\": \"might not\", \"mightn’t’ve\": \"might not have\", \"must’ve\": \"must have\", \"mustn’t\": \"must not\", \"mustn’t’ve\": \"must not have\", \"needn’t\": \"need not\", \"needn’t’ve\": \"need not have\", \"o’clock\": \"of the clock\", \"oughtn’t\": \"ought not\", \"oughtn’t’ve\": \"ought not have\", \"shan’t\": \"shall not\", \"sha’n’t\": \"shall not\", \"shan’t’ve\": \"shall not have\", \"she’d\": \"she would\", \"she’d’ve\": \"she would have\", \"She'll\": \"she will\", \"she’ll\": \"she will\", \"she’ll’ve\": \"she will have\", \"should’ve\": \"should have\", \"shouldn’t\": \"should not\", \"shouldn’t’ve\": \"should not have\", \"so’ve\": \"so have\", \"that’d\": \"that would\", \"that’d’ve\": \"that would have\", \"there’d\": \"there would\", \"there’d’ve\": \"there would have\", \"they’d\": \"they would\", \"they’d’ve\": \"they would have\",\"they’ll\": \"they will\",\n",
    "  \"they’ll’ve\": \"they will have\", \"they’re\": \"they are\", \"they’ve\": \"they have\", \"to’ve\": \"to have\", \"wasn’t\": \"was not\", \"we’d\": \"we would\", \"we’d’ve\": \"we would have\", \"we’ll\": \"we will\", \"we’ll’ve\": \"we will have\", \"we’re\": \"we are\", \"we’ve\": \"we have\", \"weren’t\": \"were not\",\"what’ll\": \"what will\", \"what’ll’ve\": \"what will have\", \"what’re\": \"what are\", \"what’ve\": \"what have\", \"when’ve\": \"when have\", \"where’d\": \"where did\", \"where’ve\": \"where have\",\n",
    "  \"who’ll\": \"who will\", \"who’ll’ve\": \"who will have\", \"who’ve\": \"who have\", \"why’ve\": \"why have\", \"will’ve\": \"will have\", \"won’t\": \"will not\", \"won’t’ve\": \"will not have\", \"would’ve\": \"would have\", \"wouldn’t\": \"would not\", \"wouldn’t’ve\": \"would not have\", \"y’all\": \"you all\", \"y’all’d\": \"you all would\", \"y’all’d’ve\": \"you all would have\", \"y’all’re\": \"you all are\", \"y’all’ve\": \"you all have\", \"you’d\": \"you would\", \"you’d’ve\": \"you would have\", \"you’ll\": \"you will\", \"You'll\": \"you will\", \"you’ll’ve\": \"you will have\", \"You're\": \"you are\", \"you’re\": \"you are\", \"you’ve\": \"you have\", \"wanna\": \"want to\", \" u \": \" you\", \" r \": \" are \", \"gawd\": \"god\", \"urlLink\": \"\", \"luv\": \"love\", \"wuts\": \"what is\", \"wasnt\": \"was not\", \"Wasnt\": \"was not\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_re = re.compile('(%s)'%'|'.join(contractions_dict.keys()))\n",
    "\n",
    "def expand_contractions(s, contractions_dict=contractions_dict):\n",
    "  def replace(match):\n",
    "    return contractions_dict[match.group(0)]\n",
    "  return contractions_re.sub(replace, s)\n",
    "\n",
    "train['post'] = train.post.apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Convert to Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowering(text):\n",
    "    return text.lower()\n",
    "\n",
    "train['post'] = train.post.apply(lowering)   \n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Remove URLs and HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all... do we really have any URL? Let's check:\n",
    "len(train[train.post.str.contains('http')]) # 10 thousands posts contain reference to websites. We need to remove these websites from the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "train['post'] = train.post.apply(remove_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train[train.post.str.contains('href=')])) # Only 88.. we can drop them\n",
    "\n",
    "def remove_html_tags_func(text):\n",
    "    return BeautifulSoup(text, 'html.parser').get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Remove strange accented characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accents(text):\n",
    "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "train['post'] = train.post.apply(remove_accents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Remove Punctuation (and Numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    # return re.sub(r'[^a-zA-Z0-9]', ' ', text) # --> if you allow for numbers\n",
    "    return re.sub(r'[^a-zA-Z]', ' ', text) # --> if you do not allow for numbers\n",
    "\n",
    "train['post'] = train.post.apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Remove extra whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_spaces(text):\n",
    "    return re.sub(r'^\\s*|\\s\\s*', ' ', text).strip()\n",
    "\n",
    "train['post'] = train.post.apply(remove_extra_spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Delete Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prima opzione: 179 parole vietate\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# sw_nltk = stopwords.words('english')\n",
    "\n",
    "# Seconda opzione: 326\n",
    "import spacy\n",
    "en = spacy.load('en_core_web_sm')\n",
    "sw_spacy = en.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete stop words\n",
    "def delete_stopwords(text):\n",
    "    words = [word for word in text.split() if word.lower() not in sw_spacy]\n",
    "    new_text = \" \".join(words)\n",
    "    return new_text\n",
    "\n",
    "# Create new columns with new text and new length\n",
    "train['post'] = train['post'].apply(delete_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ok!\n",
    "We can declare the preprocessinh phase completed.\n",
    "We can now compute the new length of the posts and have a look at the final result..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['new_length'] = train['post'].apply(post_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We save this dataset in a pickle object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as outp:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sample usage\n",
    "save_object(train, 'train_clean.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, we will read this pickle object instead of running the whole code again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>new_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ooh shiny new commenting</td>\n",
       "      <td>16</td>\n",
       "      <td>female</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>today parade suked wasnt bad band year battle ...</td>\n",
       "      <td>14</td>\n",
       "      <td>male</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>know anymore concerned everyday want bold face...</td>\n",
       "      <td>24</td>\n",
       "      <td>female</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>roof sunset posted paul</td>\n",
       "      <td>24</td>\n",
       "      <td>male</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>god love nanny absolutely greatest woman earth...</td>\n",
       "      <td>23</td>\n",
       "      <td>female</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post  age  gender  new_length\n",
       "0                           ooh shiny new commenting   16  female           4\n",
       "1  today parade suked wasnt bad band year battle ...   14    male          23\n",
       "2  know anymore concerned everyday want bold face...   24  female          38\n",
       "3                            roof sunset posted paul   24    male           4\n",
       "4  god love nanny absolutely greatest woman earth...   23  female         279"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_pickle('/Users/simonefacchiano/Desktop/Data Science/SL/Project/train_clean.pkl')\n",
    "train = train[['post', 'age', 'gender', 'new_length']]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PER SUSANNA: spostare/ eliminare questa seconda parte di EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How the length differs between males and females\n",
    "fig = px.histogram(train[train.length <= 1000], x=\"length\", color=\"gender\")\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diversa lunghezza prima e dopo il pre-processing\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=list(train[train.length < 1000].length)))\n",
    "fig.add_trace(go.Histogram(x=list(train[train.length < 1000].new_length)))\n",
    "\n",
    "# Overlay both histograms\n",
    "fig.update_layout(barmode='overlay')\n",
    "# Reduce opacity to see both histograms\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding using Sentence Transformers\n",
    "\n",
    "Our main source of inspiration:\n",
    "https://www.youtube.com/watch?v=c7AqnswslWo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "\n",
    "# Pipeline that is tipically used:\n",
    "# -   Remove punctuation                       (done)\n",
    "# -   Remove stopwords                         (done)\n",
    "# -   Implement lemmatization and tokenization (TODO)\n",
    "\n",
    "# In our particular case, it seems that we have to do the following:\n",
    "# -   Lemmatize each row\n",
    "# -   Embed using sentence transformers\n",
    "\n",
    "# At the present stage, it should be easy to do.\n",
    "# So, first thing first, we need to tokenize and lemmatize every sentence we have. \n",
    "# After this opearation, we should have the same exact structure of the dataset, but this time the posts will be a little bit different because of the lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spacy.io/models/en\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Let's create or tokenizer function\n",
    "\n",
    "def lemmatizer(sentence):\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # Lemmatize:\n",
    "    mytokens = [word.lemma_.strip() for word in doc]\n",
    "\n",
    "    # Re-join\n",
    "    sentence = \" \".join(mytokens)\n",
    "\n",
    "    # Return the sentence\n",
    "    return(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Per prima cosa, facciamo la lemmatization (su tutte le 500k righe...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if this works:\n",
    "train['lemmatize'] = train['post'].apply(lemmatizer)\n",
    "\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo salviamo in un oggetto a parte\n",
    "save_object(train, 'lemmatization.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding using sentence transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(526812, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatization = pd.read_pickle('/Users/simonefacchiano/Desktop/Data Science/SL/Project/lemmatization.pkl')\n",
    "#mini_train = lemmatization[:100000]\n",
    "\n",
    "lemmatization.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U sentence-transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "\n",
    "# https://www.sbert.net/docs/pretrained_models.html\n",
    "# According to this official website, this model (all-MiniLM-L6-v2) \"is 5 times faster and still offers good quality\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>new_length</th>\n",
       "      <th>lemmatize</th>\n",
       "      <th>age_class</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100000</th>\n",
       "      <td>love test test test non stop cant stop testing...</td>\n",
       "      <td>16</td>\n",
       "      <td>male</td>\n",
       "      <td>38</td>\n",
       "      <td>love test test test non stop can not stop test...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.057716005, -0.04005825, -0.002310888, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100001</th>\n",
       "      <td>lol yay going movieworld soon happy oh happy t...</td>\n",
       "      <td>14</td>\n",
       "      <td>female</td>\n",
       "      <td>16</td>\n",
       "      <td>lol yay go movieworld soon happy oh happy toda...</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.041464932, -0.034795586, 0.048872694, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100002</th>\n",
       "      <td>tamer ziara associated press writer rafah gaza...</td>\n",
       "      <td>27</td>\n",
       "      <td>male</td>\n",
       "      <td>79</td>\n",
       "      <td>tame ziara associated press writer rafah gaza ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.024696432, 0.036157, -0.046597987, 0.06194...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100003</th>\n",
       "      <td>society consisting men women content apply pro...</td>\n",
       "      <td>34</td>\n",
       "      <td>male</td>\n",
       "      <td>14</td>\n",
       "      <td>society consist man woman content apply progre...</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.039336678, 0.057090227, -0.011589597, -0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100004</th>\n",
       "      <td>ve blown away entire evening reaping fruits fi...</td>\n",
       "      <td>17</td>\n",
       "      <td>male</td>\n",
       "      <td>158</td>\n",
       "      <td>ve blow away entire evening reap fruit fix com...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.020673048, -0.057360034, -0.02444625, -0.00...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     post  age  gender  \\\n",
       "100000  love test test test non stop cant stop testing...   16    male   \n",
       "100001  lol yay going movieworld soon happy oh happy t...   14  female   \n",
       "100002  tamer ziara associated press writer rafah gaza...   27    male   \n",
       "100003  society consisting men women content apply pro...   34    male   \n",
       "100004  ve blown away entire evening reaping fruits fi...   17    male   \n",
       "\n",
       "        new_length                                          lemmatize  \\\n",
       "100000          38  love test test test non stop can not stop test...   \n",
       "100001          16  lol yay go movieworld soon happy oh happy toda...   \n",
       "100002          79  tame ziara associated press writer rafah gaza ...   \n",
       "100003          14  society consist man woman content apply progre...   \n",
       "100004         158  ve blow away entire evening reap fruit fix com...   \n",
       "\n",
       "        age_class                                          embedding  \n",
       "100000          0  [-0.057716005, -0.04005825, -0.002310888, -0.0...  \n",
       "100001          0  [-0.041464932, -0.034795586, 0.048872694, 0.00...  \n",
       "100002          1  [-0.024696432, 0.036157, -0.046597987, 0.06194...  \n",
       "100003          2  [0.039336678, 0.057090227, -0.011589597, -0.00...  \n",
       "100004          0  [0.020673048, -0.057360034, -0.02444625, -0.00...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.encode('Hello world')\n",
    "mini_train = lemmatization[100000:200000]\n",
    "\n",
    "mini_train['embedding'] = mini_train['lemmatize'].apply(model.encode)\n",
    "mini_train.head()\n",
    "\n",
    "# 10000 righe --> 6m 19s\n",
    "# proiezione 426k righe: 4h e mezza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(mini_train, 'embedding_100k_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sono tante righe..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = pd.read_pickle('/Users/simonefacchiano/Desktop/Data Science/SL/Project/embedding_100k.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea: e se facessimo l'embedding pezzo per pezzo?\n",
    "\n",
    "In pratica, per fare 500k righe ci metteremmo circa 6 ore... troppo.\n",
    "Ma se facessimo un pezzo alla volta? 100k righe a pranzo, 100k righe a cena ecc ecc... \n",
    "Risparmieremmo tempo ed risorse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini_train1: Sample di 500 righe\n",
    "mini_train1 = train[['post']][:500]\n",
    "\n",
    "# Lemmatization\n",
    "mini_train1['lemmatize'] = mini_train1['post'].apply(lemmatizer)\n",
    "\n",
    "# Embedding\n",
    "mini_train1['embedding'] = mini_train1['lemmatize'].apply(model.encode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini_train2: Sample di 1000 righe\n",
    "mini_train2 = train[['post']][:1000]\n",
    "\n",
    "# Lemmatization\n",
    "mini_train2['lemmatize'] = mini_train2['post'].apply(lemmatizer)\n",
    "\n",
    "# Embedding\n",
    "mini_train2['embedding'] = mini_train2['lemmatize'].apply(model.encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "1      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "2      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "3      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "4      [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "                             ...                        \n",
       "495    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "496    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "497    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "498    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "499    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
       "Name: embedding, Length: 500, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A questo punto li confronto: le prime 500 righe di mini_train2 corrispondono in tutto e per tutto a mini_train1?\n",
    "\n",
    "mini_train3 = mini_train1.embedding - mini_train2.embedding\n",
    "mini_train3[:500]\n",
    "\n",
    "# Sì! Uguale in tutto e per tutto\n",
    "# Cioò significa che potremo runnare pezzo per pezzo, senza tenerci il computer impegnato per 10 ore di fila"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
