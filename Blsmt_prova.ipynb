{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEURAL NETWORK TRIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "import gensim\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"c:\\\\Users\\\\raffl\\\\Downloads\\\\train_clean.pkl\")\n",
    "train_X, test_X, train_y, test_y = train_test_split(df['post'], df['gender'], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 70\n",
    "embed_size = 300\n",
    "max_features = 384\n",
    "\n",
    "# Supponiamo che tu abbia giÃ  creato train_X e test_X con le sequenze di testo\n",
    "\n",
    "# Inizializza il Tokenizer e tokenizza le sequenze di testo\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(train_X) + list(test_X))\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "# Esegui il padding delle sequenze di testo per avere lunghezza fissa\n",
    "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "\n",
    "# Ora train_X e test_X contengono sequenze di interi di lunghezza fissa\n",
    "# Puoi usarli per addestrare il tuo modello di deep learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "train_y = le.fit_transform(train_y.values)\n",
    "test_y = le.transform(test_y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raffl\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:3508: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "def load_glove(word_index):\n",
    "    EMBEDDING_FILE = \"c:\\\\Users\\\\raffl\\\\Desktop\\\\statistical_learning2\\\\glove.840B.300d.txt\"\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf-8\"))\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean, emb_std = -0.005838499, 0.48782197\n",
    "    embed_size = all_embs.shape[1]\n",
    "    nb_words = min(max_features, len(word_index)+1)\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            embedding_vector = embeddings_index.get(word.capitalize())\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = load_glove(tokenizer.word_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLST NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_size = 64\n",
    "        drp = 0.1\n",
    "        n_classes = len(le.classes_)\n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(embed_size, self.hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.linear = nn.Linear(self.hidden_size*4 , 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(drp)\n",
    "        self.out = nn.Linear(64, n_classes)\n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        avg_pool = torch.mean(h_lstm, 1)\n",
    "        max_pool, _ = torch.max(h_lstm, 1)\n",
    "        conc = torch.cat(( avg_pool, max_pool), 1)\n",
    "        conc = self.relu(self.linear(conc))\n",
    "        conc = self.dropout(conc)\n",
    "        out = self.out(conc)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch==1.9.0+cu111 (from versions: 1.11.0, 1.11.0+cpu, 1.11.0+cu113, 1.11.0+cu115, 1.12.0, 1.12.0+cpu, 1.12.0+cu113, 1.12.0+cu116, 1.12.1, 1.12.1+cpu, 1.12.1+cu113, 1.12.1+cu116, 1.13.0, 1.13.0+cpu, 1.13.0+cu116, 1.13.0+cu117, 1.13.1, 1.13.1+cpu, 1.13.1+cu116, 1.13.1+cu117, 2.0.0, 2.0.0+cpu, 2.0.0+cu117, 2.0.0+cu118, 2.0.1, 2.0.1+cpu, 2.0.1+cu117, 2.0.1+cu118)\n",
      "ERROR: No matching distribution found for torch==1.9.0+cu111\n"
     ]
    }
   ],
   "source": [
    "pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6 \t loss=20.9390 \t val_loss=20.7248 \t val_acc=0.6197 \t time=567.06s\n",
      "Epoch 2/6 \t loss=20.6550 \t val_loss=20.7166 \t val_acc=0.6190 \t time=556.42s\n",
      "Epoch 3/6 \t loss=20.5002 \t val_loss=20.5547 \t val_acc=0.6245 \t time=804.68s\n",
      "Epoch 4/6 \t loss=20.3661 \t val_loss=20.5468 \t val_acc=0.6238 \t time=1029.35s\n",
      "Epoch 5/6 \t loss=20.2086 \t val_loss=20.5045 \t val_acc=0.6246 \t time=657.55s\n",
      "Epoch 6/6 \t loss=20.0727 \t val_loss=20.5111 \t val_acc=0.6251 \t time=707.73s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Assuming the BiLSTM class is defined properly somewhere in your code\n",
    "\n",
    "# Make sure CUDA is available and set the device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 32\n",
    "n_epochs = 6\n",
    "model = BiLSTM().to(device)  # Move the model to the GPU if available\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "\n",
    "# Load train and test data to the CUDA Memory\n",
    "x_train = torch.tensor(train_X, dtype=torch.long).to(device)\n",
    "y_train = torch.tensor(train_y, dtype=torch.long).to(device)\n",
    "x_cv = torch.tensor(test_X, dtype=torch.long).to(device)\n",
    "y_cv = torch.tensor(test_y, dtype=torch.long).to(device)\n",
    "\n",
    "# Create Torch datasets\n",
    "train = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "valid = torch.utils.data.TensorDataset(x_cv, y_cv)\n",
    "\n",
    "# Create Data Loaders\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "    # Set model to train configuration\n",
    "    model.train()\n",
    "    avg_loss = 0.  \n",
    "    for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        # Predict/Forward Pass\n",
    "        y_pred = model(x_batch)\n",
    "        # Compute loss\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss += loss.item() / len(train_loader)\n",
    "    # Set model to validation configuration - Doesn't get trained here\n",
    "    model.eval()        \n",
    "    avg_val_loss = 0.\n",
    "    val_preds = np.zeros((len(x_cv), len(le.classes_)))\n",
    "    for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "        y_pred = model(x_batch).detach()\n",
    "        avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "        # keep/store predictions\n",
    "        val_preds[i * batch_size:(i + 1) * batch_size] = F.softmax(y_pred, dim=1).cpu().numpy()\n",
    "\n",
    "    # Check Accuracy\n",
    "    val_accuracy = sum(val_preds.argmax(axis=1) == test_y) / len(test_y)\n",
    "    train_loss.append(avg_loss)\n",
    "    valid_loss.append(avg_val_loss)\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t val_acc={:.4f} \\t time={:.2f}s'.format(\n",
    "        epoch + 1, n_epochs, avg_loss, avg_val_loss, val_accuracy, elapsed_time))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
