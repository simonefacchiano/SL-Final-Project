{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProfAIling project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries.\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from spacy import tokenizer\n",
    "from spacy.lang.en import English\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import seaborn as sns\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import pickle\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = [8, 5]\n",
    "plt.rcParams['font.family'] = 'Serif'\n",
    "our_col = ['#ef476f', '#ffd166', '#06d6a0', '#118ab2', '#073b4c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 post  age  gender\n",
      "0                           ooh shiny new commenting!   16  female\n",
      "1   so wuts up? today i had the parade. suked. but...   14    male\n",
      "2   i don't know about anyone else anymore, but i'...   24  female\n",
      "3   urlLink    another roof-top sunset  Posted by ...   24    male\n",
      "4   gawd i luv my nanny!  she's absolutely the gre...   23  female\n",
      "5   7._ Km, 39:19.4, Partly cloudy -4C, 6 km wind ...   41    male\n",
      "6   well, it's still summer vacation ':-( and I wi...   13    male\n",
      "7   Yes! School is out for summer! The sun is shin...   17  female\n",
      "8   urlLink Electric Venom:A Venomous Love Note   ...   34  female\n",
      "9   Boring mission. We had another routine presenc...   25    male\n",
      "10  I passed. Details not yet available. But I pas...   23    male\n",
      "11  Cel, this one's for you.  *Ahem*  urlLink Heh ...   27  female\n",
      "12                                           572.8!!!   27    male\n",
      "13  Eight and a half hours, at a laundromat. Gah, ...   17    male\n",
      "14  If you know what the first part of the title m...   25    male\n",
      "\n",
      "(526812, 3)\n"
     ]
    }
   ],
   "source": [
    "# Import data.\n",
    "train = pd.read_json( 'C:/Users/susan/Documents/DS/SL-Final-Project/train.json' )\n",
    "\n",
    "# Let's have a look at out data.\n",
    "print(train.head(15))\n",
    "print('')\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missin values.\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize = (10,4))\n",
    "fig_sex = sns.countplot(data=train, x='gender', palette = our_col, saturation=0.7, width=0.5, ax=axes[0])\n",
    "fig_sex.set(title='Gender distribution', xlabel='Gender', ylabel='Frequency',\n",
    "             yticklabels=(['0','50k','100k','150k','200k','250k']))\n",
    "fig_age = sns.histplot(train, x = \"age\", color=our_col[2], alpha = 0.8, binwidth=1, ax = axes[1])\n",
    "fig_age.set(title=\"Age distribution\",yticklabels=(['0','10k','20k','30k','40k','50k','60k']))\n",
    "fig.suptitle(\"Target classes distribution\",fontsize =  'xx-large')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Counts and lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New column for the length of the posts.\n",
    "train['length'] = train['post'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'M DELETING PART OF THE DATA, NEED TO BE DISCUSS!!\n",
    "train = train.loc[train[\"length\"]<80000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut in classes \n",
    "train['age_class'] = pd.cut(\n",
    "        train[\"age\"],\n",
    "        bins=[12, 18, 28, 50],\n",
    "        labels=[0, 1, 2]\n",
    "    ).astype(\"int\")\n",
    "#check\n",
    "train[\"age_class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3, figsize = (15,4))\n",
    "fig_len = sns.histplot(train[train.length <= 5000], x = \"length\", bins = 30, color=our_col[3], ax=axes[0])\n",
    "#, stat = \"frequency\" per avere le frequenze relative e non assolute\n",
    "fig_len.set(title='Histogram post length', xlabel='Length', ylabel='Frequency',\n",
    "         yticklabels=(['0','20k','40k','60k','80k','100k']), \n",
    "         xticklabels=(['0','0','1k','2k','3k','4k','5k'])\n",
    "         )\n",
    "fig_len_gen = sns.histplot(train[train.length <= 1000], x = \"length\",hue='gender', binwidth=50, palette=our_col, alpha = 0.8, multiple=\"stack\", ax = axes[1])\n",
    "#, stat = \"frequency\" per avere le frequenze relative e non assolute\n",
    "fig_len_gen.set(title='Histogram post length per gender', xlabel='Length', ylabel='Frequency',\n",
    "                yticklabels=(['0','5k','10k','15k','20k','25k','30k','35k']))\n",
    "\n",
    "fig_len_age = sns.histplot(train[train.length <= 1000], x = \"length\",hue='age_class',binwidth=25, palette=our_col[1:4], alpha = 0.8, multiple=\"stack\", ax = axes[2])\n",
    "#, stat = \"frequency\" per avere le frequenze relative e non assolute\n",
    "fig_len_age.set(title='Histogram post length per age class', xlabel='Length', ylabel='Frequency',\n",
    "                yticklabels=(['0','5k','10k','15k','20k']))\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a new colums with the number of word for the number of words in every post.\n",
    "# Different from Lenght because is not the numbers of characters.\n",
    "train['word_count'] = train['post'].apply(lambda x: len(str(x).split()))\n",
    "print(train[train['age_class']==0]['word_count'].mean()) # 12 - 17 tweets\n",
    "print(train[train['age_class']==1]['word_count'].mean()) # 18 - 29 tweets\n",
    "print(train[train['age_class']==2]['word_count'].mean()) # 29 - 50 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train.age_class==0]['word_count'].max() #115370 \n",
    "train[train.age_class==1]['word_count'].max() #69208\n",
    "train[train.age_class==2]['word_count'].max() #131169\n",
    "#weeeel very big, maybe they are outliers...\n",
    "\n",
    "# These histograms are only up to 1500 words because few tweets have more than this number but there are some tweet with a loooots of words.\n",
    "fig, axes = plt.subplots(1,3, figsize = (15,4), sharey=True)\n",
    "fig_0 = sns.histplot(train[train.age_class==0][train.word_count < 1500], x = \"word_count\", bins = 30, color=our_col[0], ax=axes[0]).set(title = \"Class 1\",yticklabels = (['0','10k','20k','30k','40k','50k','60k','70k']))\n",
    "fig_1 = sns.histplot(train[train.age_class==1][train.word_count < 1500], x = \"word_count\", bins = 30, color=our_col[1], ax=axes[1]).set(title = \"Class 2\")\n",
    "fig_2 = sns.histplot(train[train.age_class==2][train.word_count < 1500], x = \"word_count\", bins = 30, color=our_col[2], ax=axes[2]).set(title = \"Class 3\")\n",
    "fig.suptitle(\"Word count per class\",fontsize = \"xx-large\")\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence counts\n",
    "train[\"sent_count\"] = train[\"post\"].map(lambda x: len(sent_tokenize(x)))\n",
    "# Average word length\n",
    "train[\"avg_word_len\"] = train[\"post\"].map(lambda x: np.mean([len(w) for w in str(x).split()])).fillna(0)\n",
    "# Average sentence length\n",
    "train[\"avg_sent_len\"] = train[\"post\"].map(lambda x: np.mean([len(w.split()) for w in sent_tokenize(x)])).fillna(0)\n",
    "#take ages to run (8 minutes ahahah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Too many outliers, need less data.\n",
    "train_small = train.loc[train[\"length\"]<1000]\n",
    "train_small_word_count = train_small[train_small.word_count <= 230]\n",
    "train_small_sent_count = train_small[train_small.sent_count <= 30]\n",
    "train_small_avg_word = train_small[train_small.avg_word_len <= 10]\n",
    "train_small_avg_sent = train_small[train_small.avg_sent_len <= 50]\n",
    "\n",
    "# KDE plot\n",
    "# (Kernel density estimate)\n",
    "# KDE represents the data using a continuous probability density curve in one or more dimensions.\n",
    "fig, axes = plt.subplots(figsize=(17,10), ncols=3, nrows=2)\n",
    "sns.kdeplot(data = train_small, x = \"age\", ax = axes[0][0], label = \"Age\", color = our_col[0])\n",
    "sns.kdeplot(data = train_small, x = \"length\", ax = axes[0][1], label = \"Length\", hue=\"age_class\", palette = our_col)\n",
    "sns.kdeplot(data = train_small_word_count, x = \"word_count\", ax = axes[0][2], label = \"Word Count\", hue=\"age_class\", palette = our_col)\n",
    "sns.kdeplot(data = train_small_avg_sent, x = \"avg_sent_len\", ax = axes[1][0], label = \"Average sentence length\", hue=\"age_class\", palette = our_col)\n",
    "sns.kdeplot(data = train_small_avg_word, x = \"avg_word_len\", ax = axes[1][1], label = \"Average word length\", hue=\"age_class\", palette = our_col)\n",
    "sns.kdeplot(data = train_small_sent_count, x = \"sent_count\", ax = axes[1][2], label = \"Sentence count\", hue=\"age_class\", palette = our_col)\n",
    "fig.suptitle(\"KDE distribution of the features\")\n",
    "plt.tight_layout()\n",
    "\n",
    "#15 sec to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing | Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before doing anything, we save the original colum of posts in a specifi object that we will keep.\n",
    "original_posts = train['post']\n",
    "\n",
    "#mi sembra stupido, occupa solo memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also save the original length of the posts... you never know... \n",
    "\n",
    "def post_length(text):\n",
    "    return len(text.split())\n",
    "\n",
    "train['length'] = train['post'].apply(post_length) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0. Expand Contractions\n",
    "\n",
    "We noticed that it is better to expand the contractions at the begginning, before lowering the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/text-normalization-for-natural-language-processing-nlp-70a314bfa646\n",
    "# Not as efficient as other specialized packets... bu at least it works well\n",
    "contractions_dict = {\"ain’t\": \"are not\", \"'s\":\" is\", \" s \":\" is\", \"aren’t\": \"are not\", \"Aren't\": \"are not\", \"can’t\": \"cannot\", \"can’t’ve\": \"cannot have\", \"‘cause\": \"because\", \"could’ve\": \"could have\", \"couldn’t\": \"could not\", \"couldn’t’ve\": \"could not have\", \"didn’t\": \"did not\", \"doesn’t\": \"does not\", \"don't\": \"do not\", \"don t\": \"do not\", \"hadn’t\": \"had not\", \"hadn’t’ve\": \"had not have\", \"hasn’t\": \"has not\", \"haven’t\": \"have not\", \"he’d\": \"he would\", \"he’d’ve\": \"he would have\", \"he’ll\": \"he will\", \"he’ll’ve\": \"he will have\", \"how’d\": \"how did\", \"how’d’y\": \"how do you\", \"how’ll\": \"how will\", \"I’d\": \"I would\", \"I’d’ve\": \"I would have\", \"I’ll\": \"I will\", \"i'll\": \"i will\", \"'ll\":\" will\", \"I’ll’ve\": \"I will have\", \"I’m\": \"I am\", \"i'm\": \"i am\", \"'m\": \" am\", \"im\": \"i am\", \"I’ve\": \"I have\", \"i've\": \"i have\", \"isn’t\": \"is not\", \"it’d\": \"it would\", \"it’d’ve\": \"it would have\", \"it’ll\": \"it will\", \"it’ll’ve\": \"it will have\", \"let’s\": \"let us\", \"ma’am\": \"madam\", \"mayn’t\": \"may not\", \"might’ve\": \"might have\", \"mightn’t\": \"might not\", \"mightn’t’ve\": \"might not have\", \"must’ve\": \"must have\", \"mustn’t\": \"must not\", \"mustn’t’ve\": \"must not have\", \"needn’t\": \"need not\", \"needn’t’ve\": \"need not have\", \"o’clock\": \"of the clock\", \"oughtn’t\": \"ought not\", \"oughtn’t’ve\": \"ought not have\", \"shan’t\": \"shall not\", \"sha’n’t\": \"shall not\", \"shan’t’ve\": \"shall not have\", \"she’d\": \"she would\", \"she’d’ve\": \"she would have\", \"She'll\": \"she will\", \"she’ll\": \"she will\", \"she’ll’ve\": \"she will have\", \"should’ve\": \"should have\", \"shouldn’t\": \"should not\", \"shouldn’t’ve\": \"should not have\", \"so’ve\": \"so have\", \"that’d\": \"that would\", \"that’d’ve\": \"that would have\", \"there’d\": \"there would\", \"there’d’ve\": \"there would have\", \"they’d\": \"they would\", \"they’d’ve\": \"they would have\",\"they’ll\": \"they will\",\n",
    "  \"they’ll’ve\": \"they will have\", \"they’re\": \"they are\", \"they’ve\": \"they have\", \"to’ve\": \"to have\", \"wasn’t\": \"was not\", \"we’d\": \"we would\", \"we’d’ve\": \"we would have\", \"we’ll\": \"we will\", \"we’ll’ve\": \"we will have\", \"we’re\": \"we are\", \"we’ve\": \"we have\", \"weren’t\": \"were not\",\"what’ll\": \"what will\", \"what’ll’ve\": \"what will have\", \"what’re\": \"what are\", \"what’ve\": \"what have\", \"when’ve\": \"when have\", \"where’d\": \"where did\", \"where’ve\": \"where have\",\n",
    "  \"who’ll\": \"who will\", \"who’ll’ve\": \"who will have\", \"who’ve\": \"who have\", \"why’ve\": \"why have\", \"will’ve\": \"will have\", \"won’t\": \"will not\", \"won’t’ve\": \"will not have\", \"would’ve\": \"would have\", \"wouldn’t\": \"would not\", \"wouldn’t’ve\": \"would not have\", \"y’all\": \"you all\", \"y’all’d\": \"you all would\", \"y’all’d’ve\": \"you all would have\", \"y’all’re\": \"you all are\", \"y’all’ve\": \"you all have\", \"you’d\": \"you would\", \"you’d’ve\": \"you would have\", \"you’ll\": \"you will\", \"You'll\": \"you will\", \"you’ll’ve\": \"you will have\", \"You're\": \"you are\", \"you’re\": \"you are\", \"you’ve\": \"you have\", \"wanna\": \"want to\", \" u \": \" you\", \" r \": \" are \", \"gawd\": \"god\", \"urlLink\": \"\", \"luv\": \"love\", \"wuts\": \"what is\", \"wasnt\": \"was not\", \"Wasnt\": \"was not\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes 2 minutes to run\n",
    "contractions_re = re.compile('(%s)'%'|'.join(contractions_dict.keys()))\n",
    "\n",
    "def expand_contractions(s, contractions_dict=contractions_dict):\n",
    "  def replace(match):\n",
    "    return contractions_dict[match.group(0)]\n",
    "  return contractions_re.sub(replace, s)\n",
    "\n",
    "train['post'] = train.post.apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Convert to Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ooh shiny new commenting!</td>\n",
       "      <td>16</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>so what is up? today i had the parade. suked. ...</td>\n",
       "      <td>14</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i do not know about anyone else anymore, but i...</td>\n",
       "      <td>24</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>another roof-top sunset  posted by paul</td>\n",
       "      <td>24</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>god i love my nanny!  she is absolutely the gr...</td>\n",
       "      <td>23</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post  age  gender\n",
       "0                          ooh shiny new commenting!   16  female\n",
       "1  so what is up? today i had the parade. suked. ...   14    male\n",
       "2  i do not know about anyone else anymore, but i...   24  female\n",
       "3            another roof-top sunset  posted by paul   24    male\n",
       "4  god i love my nanny!  she is absolutely the gr...   23  female"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lowering(text):\n",
    "    return text.lower()\n",
    "\n",
    "train['post'] = train.post.apply(lowering)   \n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Remove URLs and HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10698\n"
     ]
    }
   ],
   "source": [
    "# First of all... do we really have any URL? Let's check:\n",
    "print(len(train[train.post.str.contains('http')])) # 10 thousands posts contain reference to websites. We need to remove these websites from the texts\n",
    "def remove_url(text):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "train['post'] = train.post.apply(remove_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(train[train.post.str.contains('href=')])) # Only 88.. we can drop them\n",
    "\n",
    "def remove_html_tags_func(text):\n",
    "    return BeautifulSoup(text, 'html.parser').get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Remove strange accented characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accents(text):\n",
    "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "train['post'] = train.post.apply(remove_accents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Remove Punctuation (and Numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    # return re.sub(r'[^a-zA-Z0-9]', ' ', text) # --> if you allow for numbers\n",
    "    return re.sub(r'[^a-zA-Z]', ' ', text) # --> if you do not allow for numbers\n",
    "\n",
    "train['post'] = train.post.apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Remove extra whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes 40 sec to run\n",
    "def remove_extra_spaces(text):\n",
    "    return re.sub(r'^\\s*|\\s\\s*', ' ', text).strip()\n",
    "\n",
    "train['post'] = train.post.apply(remove_extra_spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Delete Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prima opzione: 179 parole vietate\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# sw_nltk = stopwords.words('english')\n",
    "\n",
    "# Seconda opzione: 326\n",
    "import spacy\n",
    "en = spacy.load('en_core_web_sm')\n",
    "sw_spacy = en.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete stop words\n",
    "def delete_stopwords(text):\n",
    "    words = [word for word in text.split() if word.lower() not in sw_spacy]\n",
    "    new_text = \" \".join(words)\n",
    "    return new_text\n",
    "\n",
    "# Create new columns with new text and new length\n",
    "train['post'] = train['post'].apply(delete_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ok!\n",
    "We can declare the preprocessing phase completed.\n",
    "We can now compute the new length of the posts and have a look at the final result..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['new_length'] = train['post'].apply(post_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We save this dataset in a pickle object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as outp:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sample usage\n",
    "save_object(train, 'train_clean.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, we will read this pickle object instead of running the whole code again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('/Users/simonefacchiano/Desktop/Data Science/SL/Project/SL-Final-Project/train_clean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diversa lunghezza prima e dopo il pre-processing\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=list(train[train.length < 1000].length)))\n",
    "fig.add_trace(go.Histogram(x=list(train[train.length < 1000].new_length)))\n",
    "\n",
    "# Overlay both histograms\n",
    "fig.update_layout(barmode='overlay')\n",
    "# Reduce opacity to see both histograms\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA | Term Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_words(train):\n",
    "    corpus = [word for i in train[\"post\"].str.split().values.tolist() for word in i]\n",
    "    most_common = FreqDist(corpus).most_common(15)\n",
    "    words, frequency = [], []\n",
    "    for word, count in most_common:\n",
    "        words.append(word)\n",
    "        frequency.append(count)   \n",
    "    return words, frequency\n",
    "\n",
    "class_0 = train[train.age_class == 0]\n",
    "class_1 = train[train.age_class == 1]\n",
    "class_2 = train[train.age_class == 2]\n",
    "\n",
    "words_0,frequency_0 = most_common_words(class_0)\n",
    "words_1,frequency_1 = most_common_words(class_1)\n",
    "words_2,frequency_2 = most_common_words(class_2)\n",
    "\n",
    "class_male = train[train.gender == \"male\"]\n",
    "class_female = train[train.gender == \"female\"]\n",
    "\n",
    "words_m,frequency_m = most_common_words(class_male)\n",
    "words_f,frequency_f = most_common_words(class_female)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1,3, sharey=True, figsize=(30,5))\n",
    "plot_0 = sns.barplot(x = words_0, y = frequency_0, ax=axes[0], palette='YlGnBu').set(title=\"Class 0\" ,yticklabels = (['0','25k','50k','75k','100k','125k','150k','175k','200k']))\n",
    "plot_1 = sns.barplot(x = words_1, y = frequency_1, ax=axes[1], palette='YlOrBr').set(title=\"Class 1\")\n",
    "plot_2 = sns.barplot(x = words_2, y = frequency_2, ax=axes[2], palette='YlGn').set(title=\"Class 2\")\n",
    "fig.suptitle(\"Most common words per class\",fontsize = \"xx-large\")\n",
    "plt.tight_layout\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1,2, sharey=True, figsize=(30,5))\n",
    "plot_0 = sns.barplot(x = words_m, y = frequency_m, ax=axes[0], palette='YlGnBu').set(title=\"Male\")# ,yticklabels = (['0','25k','50k','75k','100k','125k','150k','175k','200k']))\n",
    "plot_1 = sns.barplot(x = words_f, y = frequency_f, ax=axes[1], palette='YlOrBr').set(title=\"Female\")\n",
    "fig.suptitle(\"Most common words per gender\",fontsize = \"xx-large\")\n",
    "plt.tight_layout\n",
    "\n",
    "#i maschi non hanno love! che schifo i maschi..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [word for i in train[\"post\"].str.split().values.tolist() for word in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = (pd.Series(nltk.ngrams(corpus, 2)).value_counts())[:10]\n",
    "bigrams_df = bigrams.reset_index()\n",
    "bigrams_df.columns = ['bigram', 'count']\n",
    "sns.barplot(bigrams_df, x = 'count', y = 'bigram')\n",
    "\n",
    "#2 minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = (pd.Series(nltk.ngrams(corpus, 3)).value_counts())[:10]\n",
    "trigrams_df = trigrams.reset_index()\n",
    "trigrams_df.columns = ['trigram', 'count']\n",
    "sns.barplot(trigrams_df, x = 'count', y = 'trigram')\n",
    "\n",
    "#2 minutes to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding using Sentence Transformers\n",
    "\n",
    "Our main source of inspiration:\n",
    "https://www.youtube.com/watch?v=c7AqnswslWo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creiamo una mini versione del dataset per le prove...\n",
    "\n",
    "mini_train = train[1:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "\n",
    "# Pipeline that is tipically used:\n",
    "# -   Remove punctuation                       (done)\n",
    "# -   Remove stopwords                         (done)\n",
    "# -   Implement lemmatization and tokenization (TODO)\n",
    "\n",
    "# In our particular case, it seems that we have to do the following:\n",
    "# -   Lemmatize each row\n",
    "# -   Embed using sentence transformers\n",
    "\n",
    "# At the present stage, it should be easy to do.\n",
    "# So, first thing first, we need to tokenize and lemmatize every sentence we have. \n",
    "# After this opearation, we should have the same exact structure of the dataset, but this time the posts will be a little bit different because of the lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spacy.io/models/en\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Let's create or tokenizer function\n",
    "\n",
    "def lemmatizer(sentence):\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # Lemmatize:\n",
    "    mytokens = [word.lemma_.strip() for word in doc]\n",
    "\n",
    "    # Re-join\n",
    "    sentence = \" \".join(mytokens)\n",
    "\n",
    "    # Return the sentence\n",
    "    return(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Per prima cosa, facciamo la lemmatization sull'intero dataset (più di 500k righe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_train = train[['post', 'age', 'gender', 'new_length']]\n",
    "mini_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if this works:\n",
    "mini_train['lemmatize'] = mini_train['post'].apply(lemmatizer)\n",
    "\n",
    "mini_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo salviamo in un oggetto a parte\n",
    "save_object(mini_train, 'lemmatization.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding using sentence transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatization = pd.read_pickle('/Users/simonefacchiano/Desktop/Data Science/SL/Project/SL-Final-Project/lemmatization.pkl')\n",
    "mini_train = lemmatization[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U sentence-transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "\n",
    "# https://www.sbert.net/docs/pretrained_models.html\n",
    "# According to this official website, this model (all-MiniLM-L6-v2) \"is 5 times faster and still offers good quality\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.encode('Hello world')\n",
    "\n",
    "mini_train['embedding'] = mini_train['lemmatize'].apply(model.encode)\n",
    "mini_train.head()\n",
    "\n",
    "# 10000 righe --> 6m 19s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(mini_train, 'embedding_100k.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sono tante righe..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = pd.read_pickle('/Users/simonefacchiano/Desktop/Data Science/SL/Project/embedding_100k.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "test = pd.read_json('test.json')\n",
    "\n",
    "# Let's have a look at out data\n",
    "print(test.head())\n",
    "print('')\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.age.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
