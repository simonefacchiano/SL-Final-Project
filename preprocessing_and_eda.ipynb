{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProfAIling project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries.\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from spacy import tokenizer\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "tkz = tokenizer.Tokenizer(nlp.vocab)\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "import unicodedata\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = [4, 4]\n",
    "plt.rcParams['font.family'] = 'Serif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pio.templates.default = \"simple_white\"\n",
    "px.defaults.template = \"ggplot2\"\n",
    "our_col = ['#ef476f', '#ffd166', '#06d6a0', '#118ab2', '#073b4c']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data.\n",
    "train = pd.read_json('/Users/simonefacchiano/Desktop/Data Science/SL/Project/train.json')\n",
    "\n",
    "# Let's have a look at out data.\n",
    "print(train.head(15))\n",
    "print('')\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Counts and lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New column for the length of the posts.\n",
    "train['length'] = train['post'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'M DELETING PART OF THE DATA, NEED TO BE DISCUSS!!\n",
    "train = train.loc[train[\"length\"]<80000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to eliminate post with this length\n",
    "fig = px.histogram(train, \n",
    "                   x='length',\n",
    "                   title='Histogram of Packet Length',\n",
    "                   opacity=0.8,\n",
    "                   width=800, \n",
    "                   height=400)\n",
    "fig.update_layout(title_text=\"Length\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missin values.\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target class distribution.\n",
    "#train.age.hist(bins = 15,  color = \"#118ab2\", ec=\"white\", label = \"Age distribution\")\n",
    "fig = px.histogram(train, \n",
    "                   x='age',\n",
    "                   title='Age distribution',\n",
    "                   opacity=0.8,\n",
    "                   width=800, \n",
    "                   height=400,\n",
    "                   color_discrete_sequence = [our_col[0]],\n",
    "                   )\n",
    "fig.update_layout(\n",
    "    yaxis_title_text='Frequency',\n",
    "    bargap=0.01, # gap between bars of adjacent location coordinates\n",
    "    #margin=dict(l=20, r=20, t=20, b=20),\n",
    "    #paper_bgcolor=\"gray\"\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double Check\n",
    "train['age_class'] = pd.cut(\n",
    "        train[\"age\"],\n",
    "        bins=[12, 18, 28, 50],\n",
    "        labels=[0, 1, 2]\n",
    "    ).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train[train.age <= 17]))                        # Class 1\n",
    "print(len(train[(train.age >= 23) & (train.age <= 27)]))  # Class 2\n",
    "print(len(train[(train.age >= 28) & (train.age <= 48)]))  # Class 3\n",
    "\n",
    "#same check\n",
    "train[\"age_class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a new colums with the number of word for the number of words in every post.\n",
    "# Different from Lenght because is not the numbers of characters.\n",
    "train['word_count'] = train['post'].apply(lambda x: len(str(x).split()))\n",
    "print(train[train['age_class']==0]['word_count'].mean()) # 12 - 17 tweets\n",
    "print(train[train['age_class']==1]['word_count'].mean()) # 18 - 29 tweets\n",
    "print(train[train['age_class']==2]['word_count'].mean()) # 29 - 50 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This histograms are only up to 1500 words because few tweets have more than this number but there are some tweet with a loooots of words.\n",
    "train[train.age_class==0]['word_count'].max() #115370 \n",
    "train[train.age_class==1]['word_count'].max() #69208\n",
    "train[train.age_class==2]['word_count'].max() #131169\n",
    "\n",
    "#weeeel very big, maybe they are outliers...\n",
    "word_0 = px.histogram(train[train.age_class==0][train.word_count < 1500]['word_count'],opacity=0.8,color_discrete_sequence = [our_col[0]],nbins=15)\n",
    "word_1 = px.histogram(train[train.age_class==1][train.word_count < 1500]['word_count'],opacity=0.8,color_discrete_sequence = [our_col[1]],nbins=15)\n",
    "word_2 = px.histogram(train[train.age_class==2][train.word_count < 1500]['word_count'],opacity=0.8,color_discrete_sequence = [our_col[2]],nbins=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=3,subplot_titles=(\"Class 1\", \"Class 2\", \"Class 3\"),shared_yaxes=True)\n",
    "fig.add_trace(word_0['data'][0], row=1, col=1)\n",
    "fig.add_trace(word_1['data'][0], row=1, col=2)\n",
    "fig.add_trace(word_2['data'][0], row=1, col=3)\n",
    "fig.update_layout(template='ggplot2',bargap=0.1,showlegend=False, height = 400, width = 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence counts\n",
    "train[\"sent_count\"] = train[\"post\"].map(lambda x: len(sent_tokenize(x)))\n",
    "# Average word length\n",
    "train[\"avg_word_len\"] = train[\"post\"].map(lambda x: np.mean([len(w) for w in str(x).split()])).fillna(0)\n",
    "# Average sentence length\n",
    "train[\"avg_sent_len\"] = train[\"post\"].map(lambda x: np.mean([len(w.split()) for w in sent_tokenize(x)])).fillna(0)\n",
    "\n",
    "#take ages to run (8 minutes ahahah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small = train.loc[train[\"length\"]<1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_labels = [\"Class0\",\"Class1\",\"Class2\"]\n",
    "x1_length = train_small[train_small.age_class==0]['length']\n",
    "x2_length = train_small[train_small.age_class==1]['length']\n",
    "x3_length = train_small[train_small.age_class==2]['length']\n",
    "hist_data_length = [x1_length,x2_length,x3_length]\n",
    "\n",
    "x1_word = train_small[train_small.age_class==0][train_small.word_count <= 230]['word_count']\n",
    "x2_word = train_small[train_small.age_class==1][train_small.word_count <= 230]['word_count']\n",
    "x3_word = train_small[train_small.age_class==2][train_small.word_count <= 230]['word_count']\n",
    "hist_data_word = [x1_word,x2_word,x3_word]\n",
    "\n",
    "x1_sent = train_small[train_small.age_class==0][train_small.sent_count <= 30]['sent_count']\n",
    "x2_sent = train_small[train_small.age_class==1][train_small.sent_count <= 30]['sent_count']\n",
    "x3_sent = train_small[train_small.age_class==2][train_small.sent_count <= 30]['sent_count']\n",
    "hist_data_sent = [x1_sent,x2_sent,x3_sent]\n",
    "\n",
    "\n",
    "x1_avg_w = train_small[train_small.age_class==0][train_small.avg_word_len <= 10]['avg_word_len']\n",
    "x2_avg_w = train_small[train_small.age_class==1][train_small.avg_word_len <= 10]['avg_word_len']\n",
    "x3_avg_w = train_small[train_small.age_class==2][train_small.avg_word_len <= 10]['avg_word_len']\n",
    "hist_data_avg_w = [x1_avg_w,x2_avg_w,x3_avg_w]\n",
    "\n",
    "x1_avg_s = train_small[train_small.age_class==0][train_small.avg_sent_len <= 50]['avg_sent_len']\n",
    "x2_avg_s = train_small[train_small.age_class==1][train_small.avg_sent_len <= 50]['avg_sent_len']\n",
    "x3_avg_s = train_small[train_small.age_class==2][train_small.avg_sent_len <= 50]['avg_sent_len']\n",
    "hist_data_avg_s = [x1_avg_s,x2_avg_s,x3_avg_s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = ff.create_distplot(hist_data_length, group_labels,show_hist=False, show_rug= False,colors=our_col,show_curve= True)\n",
    "word = ff.create_distplot(hist_data_word, group_labels,show_hist=False, show_rug= False,colors=our_col,show_curve= True)\n",
    "sent = ff.create_distplot(hist_data_sent, group_labels,show_hist=False, show_rug= False,colors=our_col,show_curve= True)\n",
    "avg_word = ff.create_distplot(hist_data_avg_w, group_labels,show_hist=False, show_rug= False,colors=our_col,show_curve= True)\n",
    "avg_sent = ff.create_distplot(hist_data_avg_s, group_labels,show_hist=False, show_rug= False,colors=our_col,show_curve= True)\n",
    "age = ff.create_distplot([train_small.age], group_labels=[\"Age\"],show_hist=False, show_rug= False,colors=[our_col[3]],show_curve= True)\n",
    "\n",
    "#60 seconds to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to avoid too many legends\n",
    "length['data'][0]['showlegend'] = False\n",
    "word['data'][0]['showlegend'] = False\n",
    "sent['data'][0]['showlegend'] = False\n",
    "avg_word['data'][0]['showlegend'] = False\n",
    "\n",
    "length['data'][1]['showlegend'] = False\n",
    "word['data'][1]['showlegend'] = False\n",
    "sent['data'][1]['showlegend'] = False\n",
    "avg_word['data'][1]['showlegend'] = False\n",
    "\n",
    "length['data'][2]['showlegend'] = False\n",
    "word['data'][2]['showlegend'] = False\n",
    "sent['data'][2]['showlegend'] = False\n",
    "avg_word['data'][2]['showlegend'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde = make_subplots(rows=2, cols=3,subplot_titles=(\"Age\", \"Word Count\", \"Sentence Count\", \"Avg word lenght\", \"Avg Sentence length\",\"Length\"))\n",
    "kde.add_trace(length['data'][0], row=2, col=3)\n",
    "kde.add_trace(length['data'][1], row=2, col=3)\n",
    "kde.add_trace(length['data'][2], row=2, col=3)\n",
    "\n",
    "kde.add_trace(word['data'][0], row=1, col=2)\n",
    "kde.add_trace(word['data'][1], row=1, col=2)\n",
    "kde.add_trace(word['data'][2], row=1, col=2)\n",
    "\n",
    "kde.add_trace(sent['data'][0], row=1, col=3)\n",
    "kde.add_trace(sent['data'][1], row=1, col=3)\n",
    "kde.add_trace(sent['data'][2], row=1, col=3)\n",
    "\n",
    "kde.add_trace(avg_word['data'][0], row=2, col=1)\n",
    "kde.add_trace(avg_word['data'][1], row=2, col=1)\n",
    "kde.add_trace(avg_word['data'][2], row=2, col=1)\n",
    "\n",
    "kde.add_trace(avg_sent['data'][0], row=2, col=2)\n",
    "kde.add_trace(avg_sent['data'][1], row=2, col=2)\n",
    "kde.add_trace(avg_sent['data'][2], row=2, col=2)\n",
    "kde.add_trace(age['data'][0], row=1, col=1)\n",
    "kde.update_layout(template='ggplot2')\n",
    "kde.update_layout(\n",
    "    height=1000, \n",
    "    width=1400,\n",
    "    title_text=\"KDE of the variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small_word_count = train_small[train_small.word_count <= 230]\n",
    "train_small_sent_count = train_small[train_small.sent_count <= 30]\n",
    "train_small_avg_word = train_small[train_small.avg_word_len <= 10]\n",
    "train_small_avg_sent = train_small[train_small.avg_sent_len <= 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small_word_count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE plot\n",
    "# (Kernel density estimate)\n",
    "# KDE represents the data using a continuous probability density curve in one or more dimensions.\n",
    "\n",
    "#could be cool but we need to remove outliers... \n",
    "#sns.kdeplot(data = train, x = \"length\", hue = \"age_class\")\n",
    "fig, axes = plt.subplots(figsize=(17,10), ncols=3, nrows=2)\n",
    "sns.kdeplot(data = train_small, x = \"age\", ax = axes[0][0], label = \"Age\", color = our_col[0])\n",
    "sns.kdeplot(data = train_small, x = \"length\", ax = axes[0][1], label = \"Length\", hue=\"age_class\", palette = our_col)\n",
    "sns.kdeplot(data = train_small_word_count, x = \"word_count\", ax = axes[0][2], label = \"Word Count\", hue=\"age_class\", palette = our_col)\n",
    "sns.kdeplot(data = train_small_avg_sent, x = \"avg_sent_len\", ax = axes[1][0], label = \"Average sentence length\", hue=\"age_class\", palette = our_col)\n",
    "sns.kdeplot(data = train_small_avg_word, x = \"avg_word_len\", ax = axes[1][1], label = \"Average word length\", hue=\"age_class\", palette = our_col)\n",
    "sns.kdeplot(data = train_small_sent_count, x = \"sent_count\", ax = axes[1][2], label = \"Sentence count\", hue=\"age_class\", palette = our_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for the variables or violinplots\n",
    "\n",
    "#Ora non c'ho voglia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing | Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before doing anything, we save the original colum of posts in a specifi object that we will keep.\n",
    "original_posts = train['post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also save the original length of the posts... you never know...\n",
    "\n",
    "def post_length(text):\n",
    "    return len(text.split())\n",
    "\n",
    "train['length'] = train['post'].apply(post_length) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 0. Expand Contractions\n",
    "\n",
    "We noticed that it is better to expand the contractions at the begginning, before lowering the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/text-normalization-for-natural-language-processing-nlp-70a314bfa646\n",
    "# Not as efficient as other specialized packets... bu at least it works well\n",
    "contractions_dict = {\"ain’t\": \"are not\", \"'s\":\" is\", \" s \":\" is\", \"aren’t\": \"are not\", \"Aren't\": \"are not\", \"can’t\": \"cannot\", \"can’t’ve\": \"cannot have\", \"‘cause\": \"because\", \"could’ve\": \"could have\", \"couldn’t\": \"could not\", \"couldn’t’ve\": \"could not have\", \"didn’t\": \"did not\", \"doesn’t\": \"does not\", \"don't\": \"do not\", \"don t\": \"do not\", \"hadn’t\": \"had not\", \"hadn’t’ve\": \"had not have\", \"hasn’t\": \"has not\", \"haven’t\": \"have not\", \"he’d\": \"he would\", \"he’d’ve\": \"he would have\", \"he’ll\": \"he will\", \"he’ll’ve\": \"he will have\", \"how’d\": \"how did\", \"how’d’y\": \"how do you\", \"how’ll\": \"how will\", \"I’d\": \"I would\", \"I’d’ve\": \"I would have\", \"I’ll\": \"I will\", \"i'll\": \"i will\", \"'ll\":\" will\", \"I’ll’ve\": \"I will have\", \"I’m\": \"I am\", \"i'm\": \"i am\", \"'m\": \" am\", \"im\": \"i am\", \"I’ve\": \"I have\", \"i've\": \"i have\", \"isn’t\": \"is not\", \"it’d\": \"it would\", \"it’d’ve\": \"it would have\", \"it’ll\": \"it will\", \"it’ll’ve\": \"it will have\", \"let’s\": \"let us\", \"ma’am\": \"madam\", \"mayn’t\": \"may not\", \"might’ve\": \"might have\", \"mightn’t\": \"might not\", \"mightn’t’ve\": \"might not have\", \"must’ve\": \"must have\", \"mustn’t\": \"must not\", \"mustn’t’ve\": \"must not have\", \"needn’t\": \"need not\", \"needn’t’ve\": \"need not have\", \"o’clock\": \"of the clock\", \"oughtn’t\": \"ought not\", \"oughtn’t’ve\": \"ought not have\", \"shan’t\": \"shall not\", \"sha’n’t\": \"shall not\", \"shan’t’ve\": \"shall not have\", \"she’d\": \"she would\", \"she’d’ve\": \"she would have\", \"She'll\": \"she will\", \"she’ll\": \"she will\", \"she’ll’ve\": \"she will have\", \"should’ve\": \"should have\", \"shouldn’t\": \"should not\", \"shouldn’t’ve\": \"should not have\", \"so’ve\": \"so have\", \"that’d\": \"that would\", \"that’d’ve\": \"that would have\", \"there’d\": \"there would\", \"there’d’ve\": \"there would have\", \"they’d\": \"they would\", \"they’d’ve\": \"they would have\",\"they’ll\": \"they will\",\n",
    "  \"they’ll’ve\": \"they will have\", \"they’re\": \"they are\", \"they’ve\": \"they have\", \"to’ve\": \"to have\", \"wasn’t\": \"was not\", \"we’d\": \"we would\", \"we’d’ve\": \"we would have\", \"we’ll\": \"we will\", \"we’ll’ve\": \"we will have\", \"we’re\": \"we are\", \"we’ve\": \"we have\", \"weren’t\": \"were not\",\"what’ll\": \"what will\", \"what’ll’ve\": \"what will have\", \"what’re\": \"what are\", \"what’ve\": \"what have\", \"when’ve\": \"when have\", \"where’d\": \"where did\", \"where’ve\": \"where have\",\n",
    "  \"who’ll\": \"who will\", \"who’ll’ve\": \"who will have\", \"who’ve\": \"who have\", \"why’ve\": \"why have\", \"will’ve\": \"will have\", \"won’t\": \"will not\", \"won’t’ve\": \"will not have\", \"would’ve\": \"would have\", \"wouldn’t\": \"would not\", \"wouldn’t’ve\": \"would not have\", \"y’all\": \"you all\", \"y’all’d\": \"you all would\", \"y’all’d’ve\": \"you all would have\", \"y’all’re\": \"you all are\", \"y’all’ve\": \"you all have\", \"you’d\": \"you would\", \"you’d’ve\": \"you would have\", \"you’ll\": \"you will\", \"You'll\": \"you will\", \"you’ll’ve\": \"you will have\", \"You're\": \"you are\", \"you’re\": \"you are\", \"you’ve\": \"you have\", \"wanna\": \"want to\", \" u \": \" you\", \" r \": \" are \", \"gawd\": \"god\", \"urlLink\": \"\", \"luv\": \"love\", \"wuts\": \"what is\", \"wasnt\": \"was not\", \"Wasnt\": \"was not\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_re = re.compile('(%s)'%'|'.join(contractions_dict.keys()))\n",
    "\n",
    "def expand_contractions(s, contractions_dict=contractions_dict):\n",
    "  def replace(match):\n",
    "    return contractions_dict[match.group(0)]\n",
    "  return contractions_re.sub(replace, s)\n",
    "\n",
    "train['post'] = train.post.apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Convert to Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowering(text):\n",
    "    return text.lower()\n",
    "\n",
    "train['post'] = train.post.apply(lowering)   \n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Remove URLs and HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all... do we really have any URL? Let's check:\n",
    "len(train[train.post.str.contains('http')]) # 10 thousands posts contain reference to websites. We need to remove these websites from the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "train['post'] = train.post.apply(remove_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train[train.post.str.contains('href=')])) # Only 88.. we can drop them\n",
    "\n",
    "def remove_html_tags_func(text):\n",
    "    return BeautifulSoup(text, 'html.parser').get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Remove strange accented characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accents(text):\n",
    "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "train['post'] = train.post.apply(remove_accents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Remove Punctuation (and Numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    # return re.sub(r'[^a-zA-Z0-9]', ' ', text) # --> if you allow for numbers\n",
    "    return re.sub(r'[^a-zA-Z]', ' ', text) # --> if you do not allow for numbers\n",
    "\n",
    "train['post'] = train.post.apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Remove extra whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_spaces(text):\n",
    "    return re.sub(r'^\\s*|\\s\\s*', ' ', text).strip()\n",
    "\n",
    "train['post'] = train.post.apply(remove_extra_spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Delete Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prima opzione: 179 parole vietate\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# sw_nltk = stopwords.words('english')\n",
    "\n",
    "# Seconda opzione: 326\n",
    "import spacy\n",
    "en = spacy.load('en_core_web_sm')\n",
    "sw_spacy = en.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete stop words\n",
    "def delete_stopwords(text):\n",
    "    words = [word for word in text.split() if word.lower() not in sw_spacy]\n",
    "    new_text = \" \".join(words)\n",
    "    return new_text\n",
    "\n",
    "# Create new columns with new text and new length\n",
    "train['post'] = train['post'].apply(delete_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ok!\n",
    "We can declare the preprocessinh phase completed.\n",
    "We can now compute the new length of the posts and have a look at the final result..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['new_length'] = train['post'].apply(post_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We save this dataset in a pickle object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_object(obj, filename):\n",
    "    with open(filename, 'wb') as outp:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sample usage\n",
    "save_object(train, 'train_clean.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, we will read this pickle object instead of running the whole code again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('/Users/simonefacchiano/Desktop/Data Science/SL/Project/SL-Final-Project/train_clean.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PER SUSANNA: spostare/ eliminare questa seconda parte di EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How the length differs between males and females\n",
    "fig = px.histogram(train[train.length <= 1000], x=\"length\", color=\"gender\")\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diversa lunghezza prima e dopo il pre-processing\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(x=list(train[train.length < 1000].length)))\n",
    "fig.add_trace(go.Histogram(x=list(train[train.length < 1000].new_length)))\n",
    "\n",
    "# Overlay both histograms\n",
    "fig.update_layout(barmode='overlay')\n",
    "# Reduce opacity to see both histograms\n",
    "fig.update_traces(opacity=0.75)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creiamo una mini versione del dataset per le prove...\n",
    "\n",
    "mini_train = train[1:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding using Sentence Transformers\n",
    "\n",
    "Our main source of inspiration:\n",
    "https://www.youtube.com/watch?v=c7AqnswslWo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "\n",
    "# Pipeline that is tipically used:\n",
    "# -   Remove punctuation                       (done)\n",
    "# -   Remove stopwords                         (done)\n",
    "# -   Implement lemmatization and tokenization (TODO)\n",
    "\n",
    "# In our particular case, it seems that we have to do the following:\n",
    "# -   Lemmatize each row\n",
    "# -   Embed using sentence transformers\n",
    "\n",
    "# At the present stage, it should be easy to do.\n",
    "# So, first thing first, we need to tokenize and lemmatize every sentence we have. \n",
    "# After this opearation, we should have the same exact structure of the dataset, but this time the posts will be a little bit different because of the lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spacy.io/models/en\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Let's create or tokenizer function\n",
    "\n",
    "def lemmatizer(sentence):\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # Lemmatize:\n",
    "    mytokens = [word.lemma_.strip() for word in doc]\n",
    "\n",
    "    # Re-join\n",
    "    sentence = \" \".join(mytokens)\n",
    "\n",
    "    # Return the sentence\n",
    "    return(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Per prima cosa, facciamo la lemmatization sull'intero dataset (più di 500k righe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_train = train[['post', 'age', 'gender', 'new_length']]\n",
    "mini_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if this works:\n",
    "mini_train['lemmatize'] = mini_train['post'].apply(lemmatizer)\n",
    "\n",
    "mini_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo salviamo in un oggetto a parte\n",
    "save_object(mini_train, 'lemmatization.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding using sentence transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatization = pd.read_pickle('/Users/simonefacchiano/Desktop/Data Science/SL/Project/SL-Final-Project/lemmatization.pkl')\n",
    "mini_train = lemmatization[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U sentence-transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "\n",
    "# https://www.sbert.net/docs/pretrained_models.html\n",
    "# According to this official website, this model (all-MiniLM-L6-v2) \"is 5 times faster and still offers good quality\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.encode('Hello world')\n",
    "\n",
    "mini_train['embedding'] = mini_train['lemmatize'].apply(model.encode)\n",
    "mini_train.head()\n",
    "\n",
    "# 10000 righe --> 6m 19s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_object(mini_train, 'embedding_100k.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sono tante righe..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = pd.read_pickle('/Users/simonefacchiano/Desktop/Data Science/SL/Project/embedding_100k.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "test = pd.read_json('test.json')\n",
    "\n",
    "# Let's have a look at out data\n",
    "print(test.head())\n",
    "print('')\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.age.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
